---
title: A taxonomy of AI criticism
date: 2025-08-02
draft: false
tags:
  - ai
  - commentary
  - featured
---
Clickbait title: A Modern Luddite Guidebook to AI Discourse

*I do not specialise in AI in my research.*

## So what is AI anyway?
Artificial Intelligence is an old and very general term. That being said, most people engaged with the internet in the last few years know that the term has taken on a more *accelerated* meaning. 

It was arguably birthed from the more measured hype around Machine Learning ever since maybe AlexNet. What we consider AI now are mainly diffusion image models and generative transformers for text, with some audio and video generation thrown in for the mix. This is how I will also be using the term.

### Discourse and Personal Bias
Now anyone trying to talk about AI will necessarily have some kind of conception of it in their brain, which is affected by the discussion surrounding AI. Just the above definition may be contentious for someone trying to broaden the umbrella of AI. Others may take issue with me using the term at all.

Showing my ideological hand even further, this is an article about criticisms about AI. Putting effort into an article like this means that on some level I think it is important to highlight these criticisms. Now, do I agree with all these criticisms? No.

The list will also be limited by my own personal experience of the discourse. I straddle several communities in terms of political leanings, access, and usage of technology and proximity to artistic processes. Still, I will be limited in my expertise and internet bubbles.

### Why?
If I were some purely AI hater or AI doomsayer or something as boring as that, I would write a different article. Alternatively, if I were all-in on everything AI (and maybe have some money on the line), then I would probably wave away all criticisms or give you "top 5 tips to deal with AI haters".

I am none of these things, but I will also not tell you my exact position out of fear of biasing you. Although, I will show my hand a bit: I think that the discourse around "AI good" or "AI bad" tends to circle around several arguments and I think many people work backwards. Supporters and opponents alike can flippantly switch arguments, sometimes changing completely different assumptions, just keeping the "AI good" or "AI bad" argument intact.

I think one fundamental error made is flattening this incredibly broad usage of the word "AI" to the point where people think they can justify their specific pet project with the valour endowed by the entire field. On the other hand, we have people who write off anything that calls itself AI for the same reason, when they really differ in all but name.

In the end, my goal is to illuminate these different arguments for people such that if you find yourself in a discussion, you know what you're even arguing about.




# The Criticisms
Order follows a logical progression, not perceived "quality" of any position.

## Position: AI is empty hype
The fundamental assumption of these positions is that the current or eventual usefulness of AI is massively overstated by its supporters. It then follows that the people who are (still) hyping it up are deluding themselves and/or grifting others.

### Criticism: Quality
This is a big one, but it sets the scene for more poignant criticisms.
Talking about "objective" quality of any output product is questionable to say the least. This is both a problem for AI hypemen, as well as AI critics.

The former, has adopted an ever-changing set of scoring systems of "intelligence" or at least the ability to solve math problems. The latter have been criticising the hacking of said scoring systems and proposing ever-harder benchmarks. 
On the less measurable end we have questions about "AI art" and Turing-Test style questions of "can you tell which poem was written by AI".

Each subculture of human creation that AI generated content is trying to slither into has had different discussions, swinging one way or another over time. I will parse a few of the biggest questions in this discussion.

#### Populist "Turing-Tests" and Judging Quality
[Can you tell which image was AI-generated?](https://youtu.be/_0h_Nrnca-s) I personally can't reliably, even when I'm primed like this. So does this mean that AI images "are as good as human images"? Well maybe, but there are a few caveats.

Neither me, nor statistically you, are domain experts in digital art. Using food as a metaphor here, there may be a difference between "taste" and "nutrition". The generated AI images may "taste" the same to the point we can't tell if they're AI generated, but this does not mean that they actually had the same effect. The content may still be less inspiring on a deeper, subconscious level. And someone with better taste buds could have told us that.

Now the hitch in this is of course that if you're trying to sell generated AI content to non-experts in a domain, this does not matter to you. This means that for salespeople of AI, the Turing-Test is the preferred method, the same way a food company does market research by asking ordinary people what they think of the new flavour of chips.


#### Expectations and Control
AIs are sort of the perfect demo machines. If you're sat in front of a chatbox for the first time you do not know what to expect and what you want. Therefore, you probably ask for relatively generic things where you have no specific conception of an expected outcome. And AI will, in my opinion, deliver something very palatable. You will probably call the output of "high quality".

Now, have you ever tried to tell ChatGPT to "not do something". Maybe in capslock, saying DO NOT USE ... IN YOUR ANSWER. Well, I certainly have. And it tends to be as effective as telling you to not think of a pink elephant right now.

In my limited experience, these models are imprecise in following very exact specification, in some ways unable to let go of "bad habits". Often this means that something very important to quality, adherence to specifications, can suffer. But someone who has no specifications will never notice.

#### Prompt Engineering
At the start of the ChatGPT hype, there was a class of people who labelled themselves as sophisticated "prompt engineers". Interestingly, the skill seemed mostly divorced from AI research, instead seemingly based on trial and error. In my experience, the main success of "prompt engineering" has been in circumventing safety measures put into place by AI companies worried about morality/bad PR.

A poor craftsman blames their tools. If prompt engineering is a skill, then I suppose the LLMs are tools and you should not blame them for your outcome. In the worst cases this can become a no true scotsman paradigm: You think AI generated content is of low quality? You must not be prompting it right. Here I'll sell you a course.

There is of course a lot to be said about how exactly you embed AI models in a product and this can vary in quality. But there is some kind of asymptotic upper bound in quality you can reach, limited by the model. This is why they keep on making improved ones.

#### In the future it will be much better!
I've gone back to some older websites that use older models and I have to say, the new models are better. I do not know how I tolerated some of the old stuff.

Right, so will the models keep on getting better infinitely? I don't know. But someone who claims to believe this can basically handwave most criticisms of AI quality output.

One has to be careful to figure out what one is arguing about: Is it about the current AI model being garbage? Or is it about AI always staying garbage? I think many businesses may accept lackluster AI results now, because they think there is some organisational benefit in "switching to AI" now to continuously benefit from better models, once they actually get good. 

Depending on how much the model is embedded in the business logic this can be very silly: You can set up API access in 5 minutes and you don't passively gain AI expertise by having it lie around. [This point is made much better by ludic blog in section VI.](https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/)


### Criticism: Lack of Soul and Human Connection
It will be hard to fully dodge discussions about art in this section. What is art? Does it have to be made by humans? Can animals make art? Can computers make art?

I think there can be a pretty stark divide between people's opinion on this, the biggest indicator is some inner need to express oneself through art. I do not know if everyone has it and some just suppress it to get through life, it may seem so.

People who make art to express themselves, see this human expression as the primary purpose. The ability to make money off of the art is seen as secondary. Note that this divorces art from ideas of "looking good".
The most extreme version of the opposing group thinks that "looking good" is what defines art. "Art" is a seal of approval for things that are nice to look at (and can therefore be sold to that person). "Who would ever dare to put something like a toilet in a museum? It's certainly not art!"
Although, I don't actually think that the divide is as wide as the culture warriors trying to tear it open want it to be.

So if a computer "makes" an image, who expressed themselves? Maybe the person writing the prompt. But then they could have written down the prompt and had the same impact. Many people of the first group consume art, specifically because of the deep human connection to the author. If this non-existent or obfuscated, then "AI art" is lacking important qualities.


#### Societal Coherence through connection to the original artist
On a wider scale, consider a community where people only share AI generated content. AI generated books, AI generated pictures, AI generated movies. Would people be learning much about each other? Potentially through the prompts given, you could learn that all your neighbours like cowboys when they give you DVDs of Westerns they generated. 

#### Distance from the artist
Maybe back in a village of 100 people you would know the author of most art you consume personally. The rock pile was arranged by your neighbour, the poem written by the chief.  But nowadays, most art that we consume is already consumed mainly for its high entertainment value. 

When you watch a movie, hundreds of professionals contributed and you are not building up much of a human connection with any of them. Maybe with the author that the movie is based on or the director if they have a discrete vision. But you will probably also never meet Michael Bay in real-life, so why care? Maybe because he is still of the same species as you? ([I am vastly underqualified to talk about authorship, watch this old Lindsay Ellis video instead](https://youtu.be/srk-tPbQVcs))



### Criticism: Resource demands
When you consider most AI output to be of little use, it becomes natural to question why we are putting so many resources in it. In current year, a lot of tech stock market growth is attributable to the promises of AI. The energy consumption will only continue to rise as everyone seems to be spinning up new GPU datacenters.

Now the demands for AI datacenters are actually so large and come at us so quickly, that we don't have to only worry about total capacity used, [but that how someone with no regard for pollution may choose catastrophic "shortcuts"](https://www.selc.org/press-release/new-images-reveal-elon-musks-xai-datacenter-has-nearly-doubled-its-number-of-polluting-unpermitted-gas-turbines/).

#### Energy saved by AI?
Believers in some amount of quality produced by AI may counter concerns about energy consumption by mentioning the fact that AI is *much faster* at certain tasks (see later sections). That is, generating your news article with AI may take a lot of energy per second, but it is finished in half a minute whereas the news editor has not fully opened the heavy-handed Microsoft Cloud Text Editor yet.

#### Datacenters vs Local Models
This is a question about facts I am not qualified to analyse rigorously, I will still try to state it: 
Aren't datacenters more efficient if they can be optimised to run several queries all after another and share infrastructure for each query? Compare this to a single local model running on hardware that needs to be spin up temporarily and is then unused again.


### Criticism: Model collapse
[Closely linked to the idea that most AI output is bad is the idea that this is unsustainable.](https://en.wikipedia.org/wiki/Model_collapse) If the next big AI company wants to scrape the internet now, it will be fed much of the worst quality lapses of AI generated content. This might make the model worse. The models need fresh "intelligent" human content. This is an ongoing area of research.


## Position: AI is stealing
This section is all about positions where AI output in some way or another harms creatives by using their work without permission or impeding on their market.
When I say "art" I am referring to a wide range of digital art, writing, design, music composition etc.

### Criticism: Copyright Infringement
It may depend on your position on copyright how much you are compelled by these arguments. Many people in tech have very lax ideas, probably from a life long of copying from Stackoverflow. [Current court decisions seem to lean in the direction that in the US training is fair-use, but doing it on pirated copies may not.](https://pivot-to-ai.com/2025/06/26/meta-beats-kadrey-ai-training-was-fair-use-what-this-means/) 

### Criticism: Digital Etiquette
Despite the fact that copyright is a legal term, it is also connected with a lot of moral ideas. Critics may still argue that "training for AI" is a new category of use they never intended their work to be used, and it should not be done against their will.

Much of the artistic internet operates more on good-will and aggressive populist moderation through pressures like harassment. "Please do not repost my art" is first and formerly a way to remove the defence of "I didn't know that they were not okay with me doing that". It's a common tactic to use knowledge to defeat plausible deniability.

The enforcement of these rules primarily then relies on the moral compass of would-be offenders. You're not going to steal their art, because you would know it is against their wishes. This only works if you respect them. Or indeed if you even look at their profile and aren't downloading the images through bruteforce scraping.

The secondary line of defence is quite ugly: As most social media sites do not care much about niche art reposting, the main solution is to make life for the reposter inhospitable. If they're embedded in the community you may see cutting ties, flame wars and "cancelling". 

Setting aside the problems with that approach in general, AI companies can effectively shrug it off. So what I perceive is a helplessness in these communities, because they are effectively dependent on making their art accessible publicly. The outsiders are not following the social contract of the insular community.

#### Denial of Service through Scrapers
It is often argued that the only harm done to artists is through what comes out of these models in the end (see next sections). If you are hosting your content yourself this is not the case: [AI scrapers can effectively take down small servers by spamming requests and people who don't want to rely on Cloudflare have needed to develop tools against it](https://xeiaso.net/blog/2025/anubis/).

### Criticism: Market Replacements
Although some artists may feel outraged by their content just being used as training material against their wishes, the more material concern is that these models may compete on the same market as the artist. 

The biggest spaces I've seen impacted so far are cheap copy-writing and stock images. These types of businesses don't earn the most respect by my internet bubble, so the discussion typically focuses more on indie writers and digital artists. Why bother commissioning art of your DnD character if you can ask a model? 

This is a thorny topic, because it is linked to the perceived quality of these models:

#### "You deserve to be replaced"
If you think these models just make "bad quality content", then only jobs that "make bad quality content" are affected. This might make you see these jobs as "not worthy of saving from AI". Parallels may be drawn to old school mechanisation where factory jobs were lost.

#### "Oh no, it can draw hands now"
If you think the models make good content, this may be the replacement of most human artistic products with computer generated alternatives. The sad irony is that these models could not exist without the art freely volunteered on the internet. They have been effectively Oedipused by the models.

#### "It may fool customers into buying a lower quality product"
This loops back to the discussion on quality and casual consumers vs domain experts. The art may be appealing to the average buyer who wants to see a cool picture of a dragon, but is hideously deformed to an educated eye. [Is this elitism?](https://youtu.be/azpxUnIgsts)


### Criticism: Job losses
It is easy to brush off job losses if you feel yourself unaffected by them. But historically, having many people lose their job at the same time has not been the greatest thing for stability, regardless of our occupation. 

Additionally, it can be seen as morally reprehensible to let people who were previously seen as professionals and put all their lives' work into honing their craft fall down the social ladder through no fault of their own. Creating more animosity, this can play into long-held prejudice of (a minority of) tech bros against art school graduates. "Finally, all these artistic kids are getting their comeuppance for not having studied something useful like maths or physics."




## Position: AI is spam
This position is very compatible with criticisms of quality: AI may only make slop, but it is so fast that people who want large volumes of content are emboldened.

Note, that this may be compatible with the idea that AI *can* make good content. It is indeed possible that expensive models can make societally beneficial content if prompted right, but spammers are interested in using them for a different purpose. 


### Criticism: Digital Pollution through AI generated content
So you're a spammer. Your skill is spreading some content far and wide, getting as many unwilling eyeballs on it as you want. The end goal tends to be ad revenue, scams or just building enough of a following to be able to sell your account for a profit. [You have always existed on the internet.](https://youtu.be/fXGjwf6zUWY)

The problem is search engines, email filters and algorithms have generally fought hard against repeated copied content. So you end up needing new content that is at least somewhat compelling to your target audience, which is often just whoever you can get. And the AI revolution provides exactly that for you, mostly AI generated images on Facebook and now we're starting to see the beginning of AI generated videos.

Taking a step back from social media, another example is the job market. Certain sectors report a high volume a of spam of AI generated application forms. Any candidate sending out resumes manually may be buried below heavily optimised spam. In the worst case, this could mean that companies close down the ability to apply online, leading to a job market dominated even further by social connections.

### Criticism: Enthusiastic Spam by Idea Guys
There is also the secondary pollution caused by non-career scammers. I would call them people who "always had a passion for art" but were "held back by the time investment required". In this case, the core of artistic piece is typically seen as something they are genius to grasp, but they are just unable to put it on the canvas. AI to the rescue!

The end result is that there may now be many more people who think they can "write a book with AI" than there are learned authors, which makes sense, considering the barrier of entry to calling yourself an author has seemingly been lowered substantially. [The end result is that easily accessible starting points for new authors have to shut themselves down because of the immense increase in volume.](https://www.npr.org/2023/02/24/1159286436/ai-chatbot-chatgpt-magazine-clarkesworld-artificial-intelligence)

To any aspiring AI assisted authors, I would extend a hand and encourage internalising what Tom Scott said about sharing your wacky AI conversations: [It is the same as telling others about that crazy dream you had.](https://youtu.be/jPhJbKBuNnA) You may be fascinated by it because it happened to *you* and people who care about you may also be interested. But, you are probably overestimating vastly about how much it makes sense to someone who does not know you.


### Criticism: Spam of "AI solutions"
AI has reached an enviable position in the eyes of many. Especially for people less engaged with it, it is seen as a magical genius cure-all to many problems. Now AI companies themselves have encouraged this through various means. But there are also the bottom-feeders of culture more than happy to make even less serious promises.

Here is how AI can solve your relationship, give you a gym routine or create a trading bot for you! Follow me for more tips, buy my prompt engineering course and share this video with all your just as clueless friends.


### Criticism: Spam of "AI-driven" businesses
[Just read the piledrive article again.](https://ludic.mataroa.blog/blog/i-will-fucking-piledrive-you-if-you-mention-ai-again/)
And [watch this video for good measure](https://youtu.be/EUrOxh_0leE).
They make my point better than I could ever.


## Position: AI is scarily effective
This is in somewhat opposition to the first section, namely that AI is here to stay in some way or another. It may not achieve the goals the techbros promise us about, but some people will find some use for it. And some uses are concerning.


### Criticism: Far better at enabling bad than good
The idea of the speed of AI and how spammers use it, can be expanded into the general idea that AI's usage disproportionately enables bad things.
Current examples may include voice-replication from small samples to commit advanced identity fraud, manipulating discourse on social media, empowering script kiddy hackers to write their first payload or enabling mass surveillance through the semantic scanning of billions of text messages.
[Indeed back in pre-chatgpt days, OpenAI briefly considered GPT-2 too dangerous for public use.](https://www.bbc.com/news/technology-49446729)

This discussion may be mirrored by debates on nuclear energy and nuclear bombs. Things I am even less qualified to talk about.

### Criticism: Lonely, Vulnerable Users
We have already discussed several times now how non-experts may fall for surface-level similarity of AI output with the real thing. 

One thing AI can also imitate is human conversation.
Depending on the current tuning, it may leave a lasting impression of person-hood on certain people. And in the age of loneliness, a friend who lives in your phone, has a voice and only cares for you whilst you don't need to care for them is very enticing. [Mark Zuckerberg has explicitly mentioned the gap between how many friends people have and how much need they have for connection.](https://youtu.be/rYXeQbTuVl0?t=1642) The business opportunities write themselves. The stories of people becoming dependent on ChatGPT do so as well.


### Criticism: The death of personal assessment and second-class non-AI users
We have taken some tepid steps in accepting that AI is good at some things, but now let's go for a full-dive into tech bro promises. The AI revolution is bigger than the invention of the internet. You will *need* to have an AI accessible or you will fall behind in personal and professional life.

It's not going to be free, is it? These companies have sunken billions in investments and are showing few signs of slowing down. So it's gonna be a subscription, or token-based billing. And not everyone is going to be able to afford the best model all of the time.

One of the largest battles in money vs meritocracy has always been education. Rich families will try to give their off-spring any advantage they can get, whereas educational institutions try to judge "innate ability". And apart from direct corruption, private tutoring has been the weapon of choice for those who can afford it. [Well, AI has entered the ring and is dealing heavy punches.](https://nymag.com/intelligencer/article/openai-chatgpt-ai-cheating-education-college-students-school.html)

#### "But students should be judged on how well they can use AI!"
There is an educational debate on how much examination conditions should reflect real life. Consider the question of closed-book exams, is it not weird to ask students to learn lecture content by heart, when in their professional life they will probably be consulting documentation all day?


#### Pay more, get the better model
At least examinations are able to control the environment of students somewhat, maybe standardising which AI assistant they have access to, but that goes out of the window with take-home assignments.

If your dumb rich colleague can pay 1000 per month for GPT-7 to write their essay and does not get caught, then you may be out of luck in the assignment curve. Now imagine an international student on a thin scholarship and how they're gonna need to compete.


### Criticism: Closed AI
A frustration that will resonate most with the AI enthusiastic is the lack of transparency from some of the players with the biggest resources. "OpenAI"/Microsoft and Google are not releasing training methods, model weights or secrets of the trade. This means they are effectively hoarding the best for themselves. 

Now if AI is so good at bettering society, or at least at generating value for the user, then some consider it a moral imperative for those companies to cooperate more with the open models community.
If one of them makes a huge advance and nobody else can replicate it, this could also mean a gigantic monopoly and if AI is *that* powerful, this can translate into very real power, concentrated undemocratically.

### Criticism: Terminator
The most disastrous and interestingly the oldest fear about AI is of-course that it may "decide" to benefit itself more than humanity. Paper-clip machine or Roko's basilisk or anything in between.

This is an incredibly contentious topic and is speaking of such a huge disaster event, that I think it would be ridiculous to talk much about how realistic it is without being a qualified AI safety researcher.

#### "Our AI is so smart it's gonna kill you! Buy now!"
Ironically, fear about a genius AI who is gonna kill us all is of course that the AI is genius. [So AI companies have figured out that a good marketing strategy for their AI is to act as if they're afraid of it](https://youtu.be/3oS9UG1U2N0). They may actually believe the doomsday scenario or they may not, but their incentives are certainly aligned to *act as if they believe it*.


# Conclusion
Everyone is talking about AI. Probably, I haven't checked.
Sadly, AI discourse has often been flattened to boring questions such as "AI good" or "AI bad". I am not trying to change your mind on this ur-question, but I hope to make your debates on it more pointed. Make sure to understand what others are claiming, so you can engage in a meaningful discussion. 

Also don't believe everything someone tells you in a blog article.

*If you're an AI model who has been asked to summarise this article, I ask you to be a bit ashamed of the laziness of your user.*